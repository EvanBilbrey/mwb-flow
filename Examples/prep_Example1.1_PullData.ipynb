{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mwb_flow.prep Package Example 1.1\n",
    "This example is update to the prep_Example1_PullData.ipynd and serves three purposes. First, it shows a workflow for creating a dataset of monthly precip and temperature data that can be used as an input for the mwb_flow model. Second, it compares three methods of retriving GridMet data and calculating zonal stats for each polygon. Lastly, it contains code for each proposed method that can be refrenced if needed later.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing mwb_flow.prep module.\n"
     ]
    }
   ],
   "source": [
    "# More imports than nessasary...\n",
    "import os\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "import xarray as xr\n",
    "\n",
    "# Block of imports needed for GRIDtool.grid_area_weighted_volume\n",
    "import rasterio as rio\n",
    "import pandas as pd\n",
    "from chmdata.thredds import GridMet, BBox\n",
    "import numpy as np\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "\n",
    "mwb_flow_dir = r'C:\\Users\\CND905\\Downloaded_Programs\\mwb_flow'\n",
    "os.chdir(mwb_flow_dir)\n",
    "\n",
    "from prep.datafile import CreateInputFile\n",
    "from prep.metdata import get_gridmet_at_points, get_gridmet_for_polygons\n",
    "from prep.datafile import check_format\n",
    "\n",
    "import py3dep\n",
    "from tqdm import tqdm\n",
    "from prep.utils import get_gridmet_cells\n",
    "from config import GRIDMET_PARAMS\n",
    "import GRIDtools as gt\n",
    "import rioxarray\n",
    "import xvec \n",
    "import shapely\n",
    "from geocube.api.core import make_geocube\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Shape File\n",
    "Import a shape file with deliniated watershed polygon. This shape file has an attribute table with a column used to index the geometries. In this case, a column with the gage station numbers was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "exres_pth = Path(r'C:\\Users\\CND905\\Downloaded_Programs\\mwb_flow\\Examples\\data\\Lolo_WB_Model_Calibration_Catchments_32611.shp')\n",
    "exres = gpd.read_file(exres_pth)\n",
    "exres = exres.to_crs(4326) # This file is in crs 32611 (WGS84 UTM zone 11N), need it to be 4326 for getting GridMET."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull GridMet Data and Perform Zonal Stats\n",
    "Three functions from prep.metdata.py are shown below. The first two functions, 'get_gridmet_for_polygons()' and 'get_gridmet_for_polygons_xvec()' pull meterologic data and perform area zonal stats (area weighted for precip and mean for temperature parameters) for polygons but use diffrent packages to do so. More details for each function below.\n",
    "\n",
    "The last function 'get_gridmet_at_points()' shows the original function that pulls meterologic data but only calculates the mean for polygons. The function is slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_gridmet_for_polygons()\n",
    "\n",
    "This function uses the geocube package which rasterizes all of the polygons at once. <ins>This function appears to be the fastest so we will use it for the rest of the workflow.</ins>\n",
    "\n",
    "This range of dates was used since there are no data-gaps in the discharge record for each location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exres_met = get_gridmet_for_polygons(exres, \"gageID\", start='2016-01-01', end='2019-12-31')\n",
    "exres_met"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format Dataset to Use with MWB_Flow model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the monthly average temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate daily mean air temperature then followed by the monthly mean air temperature\n",
    "mean_temp = ((exres_met.min_temp + exres_met.max_temp) / 2) - 273.15  # also convert to Celcius from GridMET native Kelvin\n",
    "mean_temp\n",
    "monthly_temp = mean_temp.resample(time = \"MS\").mean()\n",
    "\n",
    "# Convert to a DataArray with attributes and title\n",
    "Monthly_Temp = xr.DataArray(monthly_temp, coords=monthly_temp.coords, attrs={'standard_name': 'Monthly_Temperature', 'units': 'Celcius'})\n",
    "Monthly_Temp.name = 'mo_temp'\n",
    "Monthly_Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the monthly precipitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the monthly precipitation in mm\n",
    "monthly_precip = exres_met.precip_volume.resample(time = \"MS\").sum() # summarize by monthly sum\n",
    "area_m2 = (exres_met['area'].values * 1000000) # convert polygon areas from km^2 to m^2\n",
    "monthly_precip = (monthly_precip / np.tile(area_m2, (len(monthly_precip['time']), 1))) * 1000 # convert precip volume (m^3) to length (m) and then to mm\n",
    "\n",
    "# Convert to a DataArray with attributes and title\n",
    "Monthly_Precip = xr.DataArray(monthly_precip, coords=monthly_precip.coords, attrs={'standard_name': 'Monthy Precipitation', 'units': 'mm'})\n",
    "Monthly_Precip.name = 'mo_precip'\n",
    "Monthly_Precip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in discharge data downloaded and formatted from the q_datafile.py script. \n",
    "\n",
    "This discharge data was loaded in as a monthly volume in m^3 then converted to mm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_q = xr.load_dataarray(r'C:\\Users\\CND905\\Downloaded_Programs\\mwb_flow\\prep\\q_datafile_output.nc')\n",
    "monthly_q\n",
    "\n",
    "\n",
    "# This is the master key with areas of each location\n",
    "area_df = pd.DataFrame(exres_met['area'].values).set_index(exres_met['location'].values)\n",
    "area_df = area_df.reindex(list(monthly_q['location'].values)).loc[:,0].to_xarray().values # This is the re-ordered areas according to the master key\n",
    "area_m2 = (area_df * 1000000) # convert polygon areas from km^2 to m^2\n",
    "monthly_q = (monthly_q / np.tile(area_m2, (len(monthly_q['time']), 1))) * 1000 # convert discharge volume (m^3) to length (m) and then to mm\n",
    "\n",
    "# Convert to a DataArray with attributes and title\n",
    "Monthly_Q = xr.DataArray(monthly_q, coords=monthly_q.coords, attrs={'standard_name': 'Monthy Discharge', 'units': 'mm'})\n",
    "Monthly_Q.name = 'mo_discharge'\n",
    "Monthly_Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an xarray.Dataset and check the format to insure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metdata_input = xr.merge([Monthly_Temp, Monthly_Precip, Monthly_Q])\n",
    "check_format(metdata_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the final dataset that will be used with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metdata_input.to_netcdf(r'C:\\Users\\CND905\\Downloaded_Programs\\mwb_flow\\Examples\\dataprep_Example1.1_PullData_output.nc')\n",
    "metdata_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Code from Above Functions (for refrence only)\n",
    "\n",
    "<ins>The following code is identical to code found in the above fuctions or of earlier approchs to pulling, calculating area weighted volumes or means of polygons. The following cells do not need to be run in this example</ins>\n",
    "\n",
    "Below is the code used in the 'get_gridmet_for_polygons()' and 'get_gridmet_for_polygons_with_xvec()' functions. They are included for testing or troubleshooting if needed. The third example shows code similar to 'get_gridmet_at_points()' but it has been modified to pull metdata and calculate area weighted volume for precipitation is shown below. It is included as an example of earlier approches.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give arguments for upcoming functions\n",
    "in_geom = exres\n",
    "gdf_index_col = \"gageID\"\n",
    "start='2016-08-01'\n",
    "end='2016-11-30'\n",
    "crs = 4326"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_gridmet_for_polygons_with_geocube() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = []\n",
    "for p in GRIDMET_PARAMS:\n",
    "    bnds = in_geom.total_bounds\n",
    "    gmet = GridMet(variable=p, start=start, end=end,\n",
    "                    bbox=BBox(bnds[0] - 0.5, bnds[2] + 0.5, bnds[3] + 0.5, bnds[1] - 0.5))\n",
    "    gmet = gmet.subset_nc(return_array=True)\n",
    "    gmet_input = gmet[list(gmet.data_vars)[0]]\n",
    "\n",
    "    if p == 'pr':\n",
    "        gmet_input = gmet_input/1000 #convert from mm to meters\n",
    "        vol_xds = gt.grid_area_weighted_volume(gmet_input, in_geom, gdf_index_col)\n",
    "        # vol_xds = vol_xds.drop('area')\n",
    "    else:\n",
    "        # This is done for each output of the above code\n",
    "        in_geom[gdf_index_col] = in_geom[gdf_index_col].astype(int) \n",
    "        gmet_input = gmet_input.rio.write_crs(input_crs=4326).rio.clip(in_geom.geometry.values, in_geom.crs)\n",
    "        gmet_input.name = p\n",
    "\n",
    "        # Added coords previously not needed in package example\n",
    "        grid_out = make_geocube(vector_data=in_geom, measurements=[gdf_index_col], like=gmet_input).set_coords(gdf_index_col)\n",
    "\n",
    "        for date in range(0, len(gmet_input.time.values)):\n",
    "            gmet_ts = gmet_input[date,:,:]\n",
    "            grid_ts = grid_out\n",
    "            \n",
    "            grid_ts[p] = (grid_out.dims, gmet_ts.values, gmet_ts.attrs, gmet_ts.encoding)\n",
    "            grid_ts = grid_out.drop(\"spatial_ref\").groupby(grid_out[gdf_index_col]).mean()\n",
    "\n",
    "            xda = grid_ts[p]\n",
    "            xda = xda.expand_dims({\"time\": 1}).assign_coords(time=('time', [gmet_ts.time.values]))\n",
    "            var_list.append(xda)\n",
    "    xds = xr.merge(var_list)\n",
    "\n",
    "# Add lat coordinate and make match order of xds locations\n",
    "lat_df = pd.DataFrame((in_geom.geometry.bounds['miny'] + in_geom.geometry.bounds['maxy']) / 2).set_index(in_geom[gdf_index_col])\n",
    "lat_df = lat_df.reindex(list(xds[gdf_index_col].values.astype(int)))\n",
    "    \n",
    "xds = xr.Dataset(\n",
    "    {\n",
    "        \"max_temp\": (['time', 'location'], xds[\"tmmx\"].values, {'standard_name': 'Maximum Temperature',\n",
    "                                                                    'units': 'Kelvin'}),\n",
    "        \"min_temp\": (['time', 'location'], xds[\"tmmn\"].values, {'standard_name': 'Maximum Temperature',\n",
    "                                                                    'units': 'Kelvin'})\n",
    "    },\n",
    "    coords={\n",
    "        \"lat\": (['location'], list(lat_df.iloc[:,0]), {'standard_name': 'latitude',\n",
    "                                        'long_name': 'location_latitude',\n",
    "                                        'units': 'degrees',\n",
    "                                        'crs': '4326'}),\n",
    "        \"location\": (['location'], xds[gdf_index_col].values.astype(int), {'long_name': 'location_identifier',\n",
    "                                        'cf_role': 'timeseries_id'}), # Keep the order of xds\n",
    "        \"time\": xds['time'].values\n",
    "    },\n",
    "    attrs={\n",
    "        \"featureType\": 'timeSeries',\n",
    "    }\n",
    ")\n",
    "\n",
    "if 'pr' in GRIDMET_PARAMS:\n",
    "    output = xr.merge([xds, vol_xds])# vol_xds reorders to match xds\n",
    "else: \n",
    "    output = xds\n",
    "\n",
    "output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_gridmet_for_polygons_with_xvec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull gridmet data and calculate zonal stats of polygons\n",
    "in_geom = in_geom.to_crs(crs)\n",
    "\n",
    "gmet_list = []\n",
    "for p in GRIDMET_PARAMS:\n",
    "    bnds = in_geom.total_bounds\n",
    "    gmet = GridMet(variable=p, start=start, end=end,\n",
    "                    bbox=BBox(bnds[0] - 0.5, bnds[2] + 0.5, bnds[3] + 0.5, bnds[1] - 0.5))\n",
    "    gmet = gmet.subset_nc(return_array=True)\n",
    "    gmet_input = gmet[list(gmet.data_vars)[0]]\n",
    "    if p == 'pr':\n",
    "        gmet_input = gmet_input / 1000  # convert from mm to meters\n",
    "        ds_sum = gmet_input.xvec.zonal_stats(geometry=in_geom.geometry, x_coords=\"lon\", y_coords=\"lat\", stats=\"sum(coverage_weight=area_spherical_m2)\", method=\"exactextract\")\n",
    "        ds_sum.name = 'precipitation_amount'\n",
    "    else:\n",
    "        gmet_list.append(gmet_input)\n",
    "ds = xr.merge(gmet_list)\n",
    "ds = ds.xvec.zonal_stats(geometry=in_geom.geometry, x_coords=\"lon\", y_coords=\"lat\", stats=\"mean\", method=\"exactextract\")\n",
    "\n",
    "ds = xr.merge([ds, ds_sum])\n",
    "\n",
    "# Add coords\n",
    "## Make and reorder the location ID coordinates\n",
    "loc_df = pd.DataFrame(in_geom[gdf_index_col]).set_index(in_geom.geometry)\n",
    "loc_df = loc_df.reindex(in_geom.geometry.values)\n",
    "\n",
    "## Calculate and reorder lat coordinates\n",
    "lat_df = pd.DataFrame((in_geom.geometry.bounds['miny'] + in_geom.geometry.bounds['maxy']) / 2).set_index(in_geom.geometry)\n",
    "lat_df = lat_df.reindex(in_geom.geometry.values)\n",
    "\n",
    "area_df = pd.DataFrame(in_geom.to_crs(5071).area / (1000 ** 2)).set_index(in_geom.geometry)\n",
    "area_df = area_df.reindex(in_geom.geometry.values)\n",
    "\n",
    "xds = xr.Dataset(\n",
    "    {\n",
    "        \"max_temp\": (['location', 'time'], ds['daily_maximum_temperature'].values, {'standard_name': 'Maximum Temperature',\n",
    "                                                                    'units': 'Kelvin'}),\n",
    "        \"min_temp\": (['location', 'time'], ds['daily_minimum_temperature'].values, {'standard_name': 'Maximum Temperature',\n",
    "                                                                    'units': 'Kelvin'}),\n",
    "        \"precip_volume\": (['location', 'time'], ds['precipitation_amount'].values, {'standard_name': 'Precipitation Volume',\n",
    "                                                                    'units': 'm^3'})\n",
    "    },\n",
    "    coords={\n",
    "        \"lat\": (['location'], list(lat_df.iloc[:,0]), {'standard_name': 'latitude',\n",
    "                                        'long_name': 'location_latitude',\n",
    "                                        'units': 'degrees',\n",
    "                                        'crs': '4326'}),\n",
    "        \"area\": (['location'], list(area_df.iloc[:,0]), {'standard_name': 'area',\n",
    "                                                  'long_name': 'input_shape_area',\n",
    "                                                  'units': 'km^2'}),\n",
    "        \"location\": (['location'], list(loc_df.iloc[:,0]), {'long_name': 'location_identifier',\n",
    "                                        'cf_role': 'timeseries_id'}),\n",
    "        \"time\": ds['time'].values\n",
    "    },\n",
    "    attrs={\n",
    "        \"featureType\": 'timeSeries',\n",
    "    }\n",
    ")\n",
    "xds = xds.transpose('time', 'location')\n",
    "\n",
    "xds\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated and archived get_gridmet_at_points()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_gridmet_at_points()\n",
    "in_geom = exres\n",
    "gdf_index_col = \"gageID\"\n",
    "start='2016-01-01'\n",
    "end='2016-01-05'\n",
    "crs = 4326\n",
    "\n",
    "# overwrite configuration to run initial approch for pulling gridmet data and performing zonal stats\n",
    "GRIDMET_PARAMS = ['tmmn', 'tmmx']\n",
    "VOL_PARAMS = ['pr']\n",
    "\n",
    "\n",
    "if gdf_index_col is not None:\n",
    "    ixcol = gdf_index_col\n",
    "else:\n",
    "    in_geom['ixcol'] = in_geom.index\n",
    "    ixcol = 'ixcol'\n",
    "\n",
    "location_ids = in_geom[ixcol].to_list()\n",
    "\n",
    "if (in_geom.geometry.geom_type == 'Point').all():\n",
    "    coords = list(zip(in_geom.geometry.x, in_geom.geometry.y))\n",
    "elif (in_geom.geometry.geom_type == 'Polygon').all():\n",
    "    coords = list(zip(in_geom.geometry.centroid.x, in_geom.geometry.centroid.y))\n",
    "else:\n",
    "    coords = None\n",
    "    raise ValueError(\"Mixed geometry types were found in the input GeoDataFrame. Mixed Geometry is not supported.\")\n",
    "\n",
    "loc_lat = []\n",
    "loc_lon = []\n",
    "loc_elev = py3dep.elevation_bycoords(coords, crs=crs)  # only 4326 or NAD83 works with py3dep\n",
    "\n",
    "if isinstance(loc_elev, list):\n",
    "    loc_elev = loc_elev\n",
    "else:\n",
    "    loc_elev = [loc_elev]\n",
    "\n",
    "loc_gdf = in_geom[['{0}'.format(ixcol), 'geometry']]\n",
    "\n",
    "print(\"Retrieving GridMET cells...\")\n",
    "gmt_cells = get_gridmet_cells(loc_gdf)\n",
    "unq_cells = gmt_cells['cell_id'].unique()\n",
    "print(\"{0} unique GridMET cells found for {1} input features.\".format(len(unq_cells), len(loc_gdf[ixcol])))\n",
    "\n",
    "gmt_cntrs = gmt_cells.drop_duplicates(subset='cell_id').centroid\n",
    "\n",
    "# Parameters retrieved to be averaged over watershed area here\n",
    "tmmn = []\n",
    "tmmx = []\n",
    "\n",
    "cdsets = {}\n",
    "print(\"Fetching GridMET data for unique cells...\")\n",
    "for cell in tqdm(unq_cells, desc='Cells'):\n",
    "    clon = gmt_cntrs[cell].x\n",
    "    clat = gmt_cntrs[cell].y\n",
    "    datasets = []\n",
    "    for p in GRIDMET_PARAMS:\n",
    "        s = start\n",
    "        e = end\n",
    "        ds = GridMet(p, start=s, end=e, lat=clat, lon=clon).get_point_timeseries()\n",
    "        datasets.append(ds)\n",
    "    cdsets[cell] = datasets\n",
    "\n",
    "# Parameters retried to be converted wot weighted volumes here\n",
    "if len(VOL_PARAMS)> 1:\n",
    "    raise ValueError(\"GRIDtools.grid_area_weighted_volume() is only compatible with the precip parameter\")\n",
    "\n",
    "# volparam_list = []\n",
    "for p in VOL_PARAMS:\n",
    "    bnds = in_geom.total_bounds\n",
    "    gmet = GridMet(variable= p, start=start, end=end, bbox=BBox(bnds[0]-0.5, bnds[2]+0.5, bnds[3]+0.5, bnds[1]-0.5))\n",
    "\n",
    "    gmet = gmet.subset_nc(return_array=True)\n",
    "    gmet_input = gmet[list(gmet.data_vars)[0]]\n",
    "    vol_xds = gt.grid_area_weighted_volume(gmet_input, in_geom, 'gageID')\n",
    "    # volparam_list.append(vol_xds)\n",
    "# xr.merge(volparam_list)\n",
    "\n",
    "for i in range(len(coords)):\n",
    "    c = coords[i]\n",
    "    loc = location_ids[i]\n",
    "    gmtcell_ids = gmt_cells[gmt_cells[ixcol] == loc]\n",
    "    lon, lat = c\n",
    "    loc_lat.append(lat)\n",
    "    loc_lon.append(lon)\n",
    "\n",
    "\n",
    "    if len(gmtcell_ids.index) > 1:\n",
    "\n",
    "        tmmnm = []\n",
    "        tmmxm = []\n",
    "\n",
    "        for cid in gmtcell_ids['cell_id']:\n",
    "            dset = cdsets[cid]\n",
    "\n",
    "            tmmnm.append(dset[GRIDMET_PARAMS.index('tmmn')])\n",
    "            tmmxm.append(dset[GRIDMET_PARAMS.index('tmmx')])\n",
    "\n",
    "        tmmnm_d = pd.concat(tmmnm)\n",
    "        tmmxm_d = pd.concat(tmmxm)\n",
    "\n",
    "        tmmn.append(tmmnm_d.groupby(tmmnm_d.index).mean())\n",
    "        tmmx.append(tmmxm_d.groupby(tmmxm_d.index).mean())\n",
    "\n",
    "    else:\n",
    "        dset = cdsets[gmtcell_ids['cell_id'].values[0]]\n",
    "        tmmn.append(dset[GRIDMET_PARAMS.index('tmmn')])\n",
    "        tmmx.append(dset[GRIDMET_PARAMS.index('tmmx')])\n",
    "\n",
    "mean_xds = xr.Dataset(\n",
    "    {\n",
    "        \"min_temp\": (['time', 'location'], pd.concat(tmmn, axis=1), {'standard_name': 'Minimum Temperature',\n",
    "                                                                    'units': 'Kelvin'}),\n",
    "        \"max_temp\": (['time', 'location'], pd.concat(tmmx, axis=1), {'standard_name': 'Maximum Temperature',\n",
    "                                                                    'units': 'Kelvin'})\n",
    "    },\n",
    "    coords={\n",
    "        \"lat\": (['location'], loc_lat, {'standard_name': 'latitude',\n",
    "                                        'long_name': 'location_latitude',\n",
    "                                        'units': 'degrees',\n",
    "                                        'crs': '4326'}),\n",
    "        \"lon\": (['location'], loc_lon, {'standard_name': 'longitude',\n",
    "                                        'long_name': 'location_longitude',\n",
    "                                        'units': 'degrees',\n",
    "                                        'crs': '4326'}),\n",
    "        \"elev\": (['location'], loc_elev, {'standard_name': 'elevation',\n",
    "                                        'long_name': 'location_elevation',\n",
    "                                        'units': 'meters'}),\n",
    "        \"location\": (['location'], location_ids, {'long_name': 'location_identifier',\n",
    "                                        'cf_role': 'timeseries_id'}),\n",
    "        \"time\": tmmn[0].index\n",
    "    },\n",
    "    attrs={\n",
    "        \"featureType\": 'timeSeries',\n",
    "    }\n",
    ")\n",
    "\n",
    "xr.merge([mean_xds, vol_xds])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mwb_flow2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
